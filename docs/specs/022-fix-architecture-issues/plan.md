# Implementation Plan: 아키텍처 안정성 개선

**Branch**: `022-fix-architecture-issues` | **Date**: 2025-01-22 | **Spec**: [spec.md](spec.md)
**Status**: Planning - Phase 1 (Design)
**Progress**: Phase 0 (Research) → **Phase 1 (Design)** → Phase 2 (Tasks)
**Last Updated**: 2025-01-22
**Input**: Feature specification from `docs/specs/022-fix-architecture-issues/spec.md`

**Note**: This plan is generated by the `/speckit.plan` command. Phase 0 (Research) and Phase 1 (Design) artifacts are included below.

---

## 작성 가이드라인 ⚠️

**언어 사용 규칙**:
- 모든 문서는 **한글**로 작성합니다
- **기술 용어만 영어로 표기**합니다 (예: DataStore, EventBus, systemd, IPC, Accessor, Watchdog 등)
- 일반 설명, 구현 계획, 설계 결정은 모두 한글로 작성합니다

**예시**:
- ✅ 좋은 예: "DataStore Accessor 패턴을 도입하여 도메인별 접근을 제한합니다"
- ❌ 나쁜 예: "Introduce DataStore Accessor pattern to restrict domain-specific access"

---

## Summary

Research 문서 006에서 식별된 4가지 핵심 아키텍처 문제를 해결합니다:

1. **P1 (CRITICAL) - Systemd 시작 순서 경쟁 상태**: RT 프로세스가 Non-RT보다 먼저 시작되어 공유 메모리를 생성하도록 보장. systemd Before/After 지시자 수정 + Non-RT 재시도 로직 + RT sd_notify(READY=1) 신호 전송.

2. **P2 - DataStore God Object 문제**: 도메인별 Accessor 인터페이스(SensorDataAccessor, RobotStateAccessor 등) 도입으로 암묵적 결합 제거. VersionedData로 버전 관리하여 데이터 일관성 보장.

3. **P3 - EventBus 안정성**: 3단계 우선순위 큐(CRITICAL/NORMAL/DEBUG) 구현으로 이벤트 과부하 대응. 백프레셔, Throttling, Coalescing 메커니즘 추가. RT 프로세스 non-blocking 보장.

4. **P4 - HA 스플릿 브레인 방지**: systemd Watchdog 활용한 하트비트 모니터링. IPC 채널 장애 감지 및 자동 재시작 정책.

**기술적 접근**: 기존 아키텍처를 최대한 유지하며 점진적 개선. P1은 즉시 배포 가능한 hotfix, P2-P4는 점진적 마이그레이션 전략 사용.

## Technical Context

**Language/Version**: C++20
**Primary Dependencies**:
- Boost.Lockfree 1.65+ (우선순위 큐 및 SPSC 큐)
- libsystemd v240+ (sd_notify API)
- spdlog 1.x (비동기 로깅)
- prometheus-cpp 1.x (메트릭 수집)
- GoogleTest (테스트 프레임워크)

**Storage**:
- 공유 메모리 (/dev/shm) - RT/Non-RT IPC 채널
- POSIX shared memory segments (key-value DataStore)
- Lock-free queues (EventBus)

**Testing**: GoogleTest framework, AddressSanitizer, integration tests with systemd service simulation

**Target Platform**: Ubuntu 24.04 LTS with PREEMPT_RT kernel (v5.15-rt+), RT cores isolated (cores 2-3)

**Project Type**: Single embedded real-time system with RT/Non-RT process split

**Performance Goals**:
- RT 사이클 지터 < 10μs (P3 EventBus 과부하 시에도 유지)
- DataStore 버전 체크 오버헤드 < 10ns (P2 Accessor 인라인 최적화)
- EventBus 처리량: 5,000,000 msg/sec 유지
- 시스템 재시작 성공률: 100% (P1 목표)
- RT 프로세스 가동 시간: 99.9% (24시간 기준)

**Constraints**:
- RT 경로에서 메모리 할당 금지 (RAII 및 사전 할당 필수)
- RT 프로세스는 절대 블로킹되지 않음 (non-blocking event push)
- systemd Type=notify 타이밍: 공유 메모리 생성 후 1초 이내 READY 신호
- IPC 채널 복구 시간 < 10초 (P4 목표)

**Scale/Scope**:
- DataStore 키 개수: ~100개 (5개 도메인 × 20개 평균)
- EventBus 큐 크기: 4096 entries (3단계 우선순위)
- 영향 받는 소스 파일: ~30개 (점진적 마이그레이션)
- 새로운 테스트: ~20개 (각 우선순위별 독립 테스트)

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### GATE 1: 계층적 아키텍처 원칙 (Principle I)
- ✅ **PASS**: 본 Feature는 기존 3계층 구조를 유지하며, 각 계층의 책임을 더 명확히 분리합니다
  - DataStore Accessor는 기존 DataStore를 감싸는 뷰(view)이므로 새로운 계층 추가 아님
  - EventBus 우선순위 큐는 기존 EventBus 내부 개선이므로 계층 변경 없음
  - systemd 시작 순서는 프로세스 수준 설정이므로 아키텍처 계층 무관

### GATE 2: 인터페이스 기반 설계 (Principle II)
- ✅ **PASS**: 모든 새로운 확장 지점은 인터페이스로 정의됩니다
  - `IDataAccessor` 인터페이스 (I-prefix 준수)
  - `ISensorDataAccessor`, `IRobotStateAccessor` 등 도메인별 인터페이스
  - 기존 `IDataStore` 인터페이스는 그대로 유지

### GATE 3: RAII 원칙 (Principle III - NON-NEGOTIABLE)
- ✅ **PASS**: 모든 리소스는 RAII로 관리됩니다
  - VersionedData는 스택 할당 구조체 (소멸자 자동 호출)
  - Accessor는 DataStore 참조만 보유 (소유권 없음)
  - EventBus 큐는 std::unique_ptr로 관리 (기존 패턴 유지)

### GATE 4: 메모리 안전성 (Principle IV - NON-NEGOTIABLE)
- ✅ **PASS**: AddressSanitizer 활성화 및 모든 테스트에서 검증
  - 새로운 코드는 모두 스마트 포인터 사용
  - 수동 메모리 관리 없음
  - 통합 테스트에서 메모리 누수 자동 감지

### GATE 5: 테스트 주도 개발 (Principle V)
- ✅ **PASS**: 모든 우선순위(P1-P4)에 대해 테스트 먼저 작성
  - P1: systemd 시작 순서 통합 테스트 (10회 재시작 검증)
  - P2: Accessor 강제 사용 단위 테스트
  - P3: EventBus 과부하 시나리오 테스트
  - P4: IPC 채널 장애 시뮬레이션 테스트

### GATE 6: 실시간 성능 (Principle VI)
- ✅ **PASS**: RT 성능 목표 유지
  - RT 사이클 지터 < 10μs (P3 EventBus non-blocking 보장)
  - DataStore Accessor 인라인 함수 (오버헤드 < 10ns)
  - EventBus push는 lock-free (기존 SPSC 큐 유지)

### GATE 7: 문서화 및 한글 사용 (Principle VII)
- ✅ **PASS**: 모든 문서는 한글로 작성되며, 기술 용어만 영어 표기
  - spec.md, plan.md, tasks.md 모두 한글 작성
  - 기술 용어: DataStore, EventBus, systemd, Accessor, Watchdog 등

**종합 평가**: ✅ **ALL GATES PASSED** - 구현 진행 승인

## Project Structure

### Documentation (this feature)

```text
docs/specs/022-fix-architecture-issues/
├── plan.md              # This file (/speckit.plan command output)
├── research.md          # Phase 0 output (아래 Phase 0 섹션 참조)
├── data-model.md        # Phase 1 output (아래 Phase 1 섹션 참조)
├── quickstart.md        # Phase 1 output (아래 Phase 1 섹션 참조)
├── contracts/           # Phase 1 output (API/Data schemas)
│   ├── data-accessor-interface.h      # IDataAccessor 인터페이스 정의
│   ├── sensor-data-accessor.h         # ISensorDataAccessor 명세
│   ├── robot-state-accessor.h         # IRobotStateAccessor 명세
│   ├── versioned-data.h               # VersionedData 구조체 명세
│   ├── event-priority.h               # EventPriority enum 정의
│   └── data-contracts.md              # DataStore 스키마 문서
└── tasks.md             # Phase 2 output (/speckit.tasks command - NOT created by /speckit.plan)
```

### Source Code (repository root)

```text
src/core/
├── datastore/
│   ├── interfaces/
│   │   ├── IDataStore.h              # 기존 인터페이스 (유지)
│   │   ├── IDataAccessor.h           # 새로운 기본 Accessor 인터페이스 (P2)
│   │   ├── ISensorDataAccessor.h     # 센서 데이터 Accessor (P2)
│   │   ├── IRobotStateAccessor.h     # 로봇 상태 Accessor (P2)
│   │   └── ITaskStatusAccessor.h     # Task 상태 Accessor (P2)
│   ├── core/
│   │   └── VersionedData.h           # 버전 관리 데이터 래퍼 (P2)
│   ├── impl/
│   │   ├── SensorDataAccessor.cpp    # Sensor Accessor 구현 (P2)
│   │   ├── RobotStateAccessor.cpp    # RobotState Accessor 구현 (P2)
│   │   └── TaskStatusAccessor.cpp    # TaskStatus Accessor 구현 (P2)
│   └── managers/
│       └── DataStore.cpp             # 기존 DataStore (VersionedData 통합)
│
├── event/
│   ├── interfaces/
│   │   └── IEventBus.h               # 기존 인터페이스 (Priority 추가)
│   ├── core/
│   │   ├── EventPriority.h           # Priority enum (CRITICAL/NORMAL/DEBUG) (P3)
│   │   ├── PrioritizedEvent.h        # 우선순위 이벤트 구조체 (P3)
│   │   └── PriorityQueue.h           # 3단계 우선순위 큐 (P3)
│   ├── adapters/
│   │   ├── EventBus.cpp              # 기존 EventBus (Priority Queue 통합)
│   │   ├── ThrottlingPolicy.cpp      # Throttling 메커니즘 (P3)
│   │   └── CoalescingPolicy.cpp      # Event Coalescing (P3)
│   └── util/
│       └── BackpressureMonitor.cpp   # 백프레셔 모니터링 (P3)
│
├── rt/
│   └── ipc/
│       ├── SharedMemoryManager.cpp   # 기존 (재시도 로직 추가) (P1)
│       └── IPCInitializer.cpp        # 새로운 초기화 로직 (P1)
│
└── systemd/
    ├── interfaces/
    │   └── IWatchdogNotifier.h       # Watchdog 인터페이스 (P4)
    └── impl/
        ├── WatchdogNotifier.cpp      # sd_notify() 래퍼 (P1, P4)
        └── StartupNotifier.cpp       # READY=1 신호 전송 (P1)

systemd/
├── mxrc-rt.service                   # RT 서비스 (Before 지시자 수정) (P1)
└── mxrc-nonrt.service                # Non-RT 서비스 (After 지시자 수정) (P1)

tests/
├── unit/
│   ├── datastore/
│   │   ├── VersionedDataTest.cpp     # 버전 관리 테스트 (P2)
│   │   ├── SensorAccessorTest.cpp    # Sensor Accessor 테스트 (P2)
│   │   └── RobotStateAccessorTest.cpp # RobotState Accessor 테스트 (P2)
│   ├── event/
│   │   ├── PriorityQueueTest.cpp     # 우선순위 큐 테스트 (P3)
│   │   ├── ThrottlingTest.cpp        # Throttling 테스트 (P3)
│   │   └── CoalescingTest.cpp        # Coalescing 테스트 (P3)
│   └── systemd/
│       ├── StartupNotifierTest.cpp   # sd_notify 테스트 (P1)
│       └── WatchdogTest.cpp          # Watchdog 테스트 (P4)
│
└── integration/
    ├── systemd/
    │   ├── startup_order_test.cpp    # 시작 순서 테스트 (P1)
    │   ├── retry_logic_test.cpp      # 재시도 로직 테스트 (P1)
    │   └── watchdog_recovery_test.cpp # Watchdog 복구 테스트 (P4)
    ├── datastore/
    │   └── version_consistency_test.cpp # 버전 일관성 테스트 (P2)
    └── event/
        └── event_storm_test.cpp      # 이벤트 폭주 테스트 (P3)
```

**Structure Decision**: 단일 프로젝트 구조 유지. 기존 `src/core/` 아래 모듈별 디렉토리 구조를 그대로 사용하며, 각 모듈 내부에 새로운 클래스 추가. systemd 서비스 파일은 `systemd/` 디렉토리에 위치 (기존 Feature 018 구조 재활용).

## Complexity Tracking

> **본 Feature는 Constitution 위반이 없으므로 이 섹션은 비어 있습니다.**

모든 GATE를 통과했으며, 정당화가 필요한 복잡도 증가 사항이 없습니다.

---

## Phase 0: Research & Decision Log

### Research Task 1: systemd Before/After 지시자 동작 방식

**목적**: RT 프로세스가 Non-RT보다 먼저 시작되도록 보장하는 systemd 설정 방법 연구

**조사 내용**:
- **Before= 지시자**: 해당 유닛이 지정된 유닛보다 **먼저** 시작됨을 보장
- **After= 지시자**: 해당 유닛이 지정된 유닛보다 **나중에** 시작됨을 보장
- **Wants=/Requires=**: 종속성은 정의하지만 시작 순서는 보장하지 않음 (After/Before 필요)

**현재 상태 분석**:
```ini
# mxrc-nonrt.service (현재)
[Unit]
After=network.target
Before=mxrc-rt.service  # ❌ 문제: Non-RT가 RT보다 먼저 시작

# mxrc-rt.service (현재)
[Unit]
After=network.target mxrc-nonrt.service  # ❌ RT가 Non-RT 다음에 시작
```

**결정**: 다음과 같이 수정 필요
```ini
# mxrc-rt.service (수정 후)
[Unit]
After=network.target
Before=mxrc-nonrt.service  # ✅ RT가 Non-RT보다 먼저 시작

# mxrc-nonrt.service (수정 후)
[Unit]
After=network.target mxrc-rt.service  # ✅ Non-RT가 RT 다음에 시작
```

**근거**: [systemd.unit(5)](https://www.freedesktop.org/software/systemd/man/systemd.unit.html) 공식 문서

---

### Research Task 2: sd_notify() READY=1 신호 타이밍

**목적**: RT 프로세스가 공유 메모리 생성 후 systemd에 READY 신호를 보내는 시점 결정

**조사 내용**:
- **Type=notify**: sd_notify(0, "READY=1") 호출 전까지 systemd가 서비스를 "starting" 상태로 유지
- **TimeoutStartSec**: READY 신호를 받지 못하면 타임아웃 발생 (현재 30초)
- **Before= 지시자와 조합**: READY 신호를 받은 후에야 Before 지시자가 의존하는 서비스(Non-RT) 시작

**최적 타이밍 결정**:
```cpp
// RT 프로세스 초기화 순서
void RTExecutive::initialize() {
    // 1. RT 스케줄링 설정
    setupRTScheduling();

    // 2. 공유 메모리 생성 (CRITICAL - Non-RT가 의존)
    createSharedMemory();

    // 3. IPC 채널 초기화
    initializeIPC();

    // 4. ✅ READY 신호 전송 (공유 메모리 준비 완료)
    sd_notify(0, "READY=1");

    // 5. 나머지 초기화 (백그라운드)
    initializeMonitoring();
    initializeMetrics();
}
```

**결정**: 공유 메모리 생성 및 IPC 채널 초기화 완료 직후 READY 신호 전송

**근거**: [sd_notify(3)](https://www.freedesktop.org/software/systemd/man/sd_notify.html) 및 Feature 018 quickstart.md 부팅 최적화 섹션

---

### Research Task 3: Non-RT 재시도 로직 설계

**목적**: Non-RT 프로세스가 공유 메모리 연결 실패 시 재시도하는 방법 연구

**조사 내용**:
- **Exponential Backoff**: 재시도 간격을 지수적으로 증가 (예: 10ms → 20ms → 40ms → ...)
- **Fixed Interval**: 일정한 간격으로 재시도 (예: 100ms 고정)
- **Timeout 설정**: 최대 재시도 시간 제한

**결정**: **Fixed Interval (100ms)** 방식 채택
- **재시도 간격**: 100ms 고정
- **최대 재시도 시간**: 5초 (50회 시도)
- **이유**:
  - Exponential Backoff는 RT 프로세스 준비 시간이 예측 가능한 경우 불필요
  - RT 프로세스는 일반적으로 1초 이내 READY 신호 전송
  - 100ms는 CPU 부하를 주지 않으면서도 빠른 연결 보장

```cpp
// Non-RT 초기화 로직
bool NonRTExecutive::connectSharedMemory() {
    const int MAX_RETRIES = 50;          // 5초 (100ms × 50)
    const int RETRY_INTERVAL_MS = 100;

    for (int i = 0; i < MAX_RETRIES; ++i) {
        if (tryOpenSharedMemory()) {
            spdlog::info("Connected to shared memory on attempt {}", i + 1);
            return true;
        }

        if (i < MAX_RETRIES - 1) {
            std::this_thread::sleep_for(std::chrono::milliseconds(RETRY_INTERVAL_MS));
        }
    }

    spdlog::error("Failed to connect to shared memory after {} attempts", MAX_RETRIES);
    return false;
}
```

**대안 거부**: Exponential Backoff는 네트워크 재연결 시 유용하지만, 로컬 공유 메모리에는 과도한 복잡도 증가

---

### Research Task 4: DataStore Accessor 패턴 설계

**목적**: 도메인별 Accessor 인터페이스 설계 및 타입 안전성 보장 방법 연구

**조사 내용**:
- **Facade 패턴**: DataStore에 대한 제한된 뷰(view) 제공
- **Template Specialization**: 도메인별 타입을 템플릿으로 정의
- **Compile-time Type Safety**: 잘못된 키 접근 시 컴파일 오류 발생

**결정**: **Facade 패턴 + Constexpr Key Validation** 조합
```cpp
// IDataAccessor.h (기본 인터페이스)
class IDataAccessor {
public:
    virtual ~IDataAccessor() = default;
    virtual std::string getDomain() const = 0;
};

// ISensorDataAccessor.h (도메인별 인터페이스)
class ISensorDataAccessor : public IDataAccessor {
public:
    virtual VersionedData<double> getTemperature() const = 0;
    virtual VersionedData<double> getPressure() const = 0;
    virtual void setTemperature(double value) = 0;
    // ✅ 다른 도메인 키(예: robot_state)는 접근 불가
};

// SensorDataAccessor.cpp (구현체)
class SensorDataAccessor : public ISensorDataAccessor {
private:
    IDataStore& datastore_;  // 참조로 보유 (소유권 없음)

    // Compile-time key validation
    static constexpr std::array<const char*, 5> ALLOWED_KEYS = {
        "sensor.temperature",
        "sensor.pressure",
        "sensor.humidity",
        "sensor.vibration",
        "sensor.current"
    };

public:
    VersionedData<double> getTemperature() const override {
        return datastore_.getVersioned<double>("sensor.temperature");
    }

    void setTemperature(double value) override {
        datastore_.setVersioned("sensor.temperature", value);
    }
};
```

**장점**:
- 도메인별 인터페이스로 의존성 명확화
- 컴파일 타임 타입 안전성 (잘못된 키 접근 시 오류)
- DataStore 소유권 없음 (참조만 보유, RAII 준수)

**대안 거부**: Template Specialization은 유연하지만, 인터페이스 기반 설계 원칙(Principle II) 위반

---

### Research Task 5: VersionedData 구조체 설계

**목적**: 데이터 버전 관리 메커니즘 설계 (일관성 검증)

**조사 내용**:
- **Sequence Number**: 단조 증가하는 uint64_t 카운터
- **Timestamp**: std::chrono::steady_clock (monotonic time)
- **Double-Buffering**: 읽는 동안 쓰기 가능 (lock-free)

**결정**: **Sequence Number + Timestamp** 조합
```cpp
template <typename T>
struct VersionedData {
    T value;
    uint64_t version;        // 단조 증가 (atomic increment)
    uint64_t timestamp_ns;   // nanosecond precision

    // 버전 일관성 검증
    bool isConsistentWith(const VersionedData& other) const {
        return version == other.version;
    }

    // 최신 여부 확인
    bool isNewerThan(const VersionedData& other) const {
        return version > other.version;
    }
};

// DataStore에서 버전 증가
template <typename T>
void DataStore::setVersioned(const std::string& key, const T& value) {
    auto& entry = data_[key];
    entry.value = value;
    entry.version++;  // atomic increment
    entry.timestamp_ns = std::chrono::steady_clock::now().time_since_epoch().count();
}
```

**버전 불일치 처리 전략**:
```cpp
// Non-RT에서 일관된 스냅샷 읽기
bool NonRTExecutive::readConsistentSnapshot() {
    const int MAX_RETRIES = 3;

    for (int i = 0; i < MAX_RETRIES; ++i) {
        auto temp1 = accessor->getTemperature();
        auto pressure1 = accessor->getPressure();
        auto temp2 = accessor->getTemperature();  // 재확인

        if (temp1.version == temp2.version) {
            // ✅ 일관성 확인됨
            processData(temp1.value, pressure1.value);
            return true;
        }

        // ⚠️ 버전 불일치 감지 - 재시도
        spdlog::warn("Version mismatch detected, retry {}/{}", i+1, MAX_RETRIES);
    }

    // 3회 실패 시 최신 버전 강제 사용
    spdlog::error("Version mismatch persists, using latest version");
    auto latest = accessor->getTemperature();
    processData(latest.value, latest.value);  // 강제 사용
    return false;
}
```

**근거**: Linux kernel의 seqlock 메커니즘 참조 (reader-writer lock-free)

---

### Research Task 6: EventBus 우선순위 큐 구현

**목적**: 3단계 우선순위 큐(CRITICAL/NORMAL/DEBUG) 및 백프레셔 메커니즘 설계

**조사 내용**:
- **Multi-level Queue**: 각 우선순위별 독립 큐
- **Priority Inheritance**: 높은 우선순위 이벤트가 낮은 우선순위 차단 방지
- **Drop Policy**: 큐 포화 시 낮은 우선순위부터 버림

**결정**: **3개의 Boost.Lockfree SPSC Queue 조합**
```cpp
enum class EventPriority {
    CRITICAL = 0,  // 오류, 상태 변경
    NORMAL = 1,    // 일반 이벤트
    DEBUG = 2      // 디버그 로그
};

class PriorityQueue {
private:
    using Queue = boost::lockfree::spsc_queue<PrioritizedEvent>;

    std::array<std::unique_ptr<Queue>, 3> queues_;
    std::atomic<size_t> total_size_{0};

    const size_t QUEUE_CAPACITY = 4096;
    const size_t DROP_THRESHOLD = 3276;  // 80% of 4096

public:
    // Non-blocking push (RT 프로세스용)
    bool push(PrioritizedEvent&& event) {
        size_t current_size = total_size_.load(std::memory_order_relaxed);

        // 백프레셔: 80% 이상 차면 DEBUG부터 버림
        if (current_size > DROP_THRESHOLD) {
            if (event.priority == EventPriority::DEBUG) {
                metrics_.debugEventsDropped++;
                return false;  // ✅ Non-blocking (즉시 반환)
            }
            if (current_size > DROP_THRESHOLD * 1.1) {  // 90% 이상
                if (event.priority == EventPriority::NORMAL) {
                    metrics_.normalEventsDropped++;
                    return false;
                }
            }
        }

        // CRITICAL은 항상 큐에 추가 시도
        int priority_idx = static_cast<int>(event.priority);
        if (queues_[priority_idx]->push(std::move(event))) {
            total_size_.fetch_add(1, std::memory_order_relaxed);
            return true;
        }

        return false;  // 큐 가득 참
    }

    // Pop (CRITICAL 우선, NORMAL, DEBUG 순서)
    std::optional<PrioritizedEvent> pop() {
        for (int i = 0; i < 3; ++i) {
            PrioritizedEvent event;
            if (queues_[i]->pop(event)) {
                total_size_.fetch_sub(1, std::memory_order_relaxed);
                return event;
            }
        }
        return std::nullopt;
    }
};
```

**Throttling 메커니즘**:
```cpp
class ThrottlingPolicy {
private:
    std::unordered_map<std::string, uint64_t> last_sent_time_;
    const uint64_t THROTTLE_INTERVAL_MS = 100;  // 100ms

public:
    bool shouldSend(const std::string& event_type) {
        uint64_t now = getCurrentTimeMs();
        auto it = last_sent_time_.find(event_type);

        if (it == last_sent_time_.end() || (now - it->second) > THROTTLE_INTERVAL_MS) {
            last_sent_time_[event_type] = now;
            return true;
        }

        return false;  // Throttled
    }
};
```

**근거**: Boost.Lockfree SPSC Queue는 이미 Feature 021에서 검증됨

---

### Research Task 7: systemd Watchdog 설정 및 하트비트 타이밍

**목적**: Watchdog 타임아웃 및 하트비트 전송 주기 결정

**조사 내용**:
- **WatchdogSec**: Watchdog 타임아웃 (현재 30초, Feature 018 설정)
- **sd_notify(WATCHDOG=1)**: 하트비트 신호
- **권장 비율**: 하트비트 주기 = WatchdogSec / 3 (안전 마진)

**결정**:
- **WatchdogSec**: 30초 유지 (기존 Feature 018 설정)
- **하트비트 주기**: 10초 (30초 / 3)
- **전송 스레드**: Non-RT 스레드에서 주기적 전송

```cpp
class WatchdogNotifier {
private:
    std::atomic<bool> running_{false};
    std::thread watchdog_thread_;
    const std::chrono::seconds WATCHDOG_INTERVAL{10};  // 10초

public:
    void start() {
        running_ = true;
        watchdog_thread_ = std::thread([this]() {
            while (running_) {
                sd_notify(0, "WATCHDOG=1");
                std::this_thread::sleep_for(WATCHDOG_INTERVAL);
            }
        });
    }

    void stop() {
        running_ = false;
        if (watchdog_thread_.joinable()) {
            watchdog_thread_.join();
        }
    }
};
```

**IPC 채널 장애 감지**:
```cpp
// RT 프로세스에서 IPC 상태 확인
bool RTExecutive::checkIPCHealth() {
    if (!ipc_channel_->isHealthy()) {
        spdlog::critical("IPC channel failure detected - Watchdog will trigger restart");
        // Watchdog가 타임아웃되도록 하트비트 중단
        watchdog_notifier_->stop();
        return false;
    }
    return true;
}
```

**근거**: Feature 018 mxrc-rt.service의 WatchdogSec=30s 설정

---

### Research Task 8: Prometheus 메트릭 정의

**목적**: 아키텍처 안정성 관련 메트릭 식별

**결정**: 다음 메트릭 추가
```cpp
// P1 - Systemd 시작 순서
prometheus::Counter& startup_failures_;          // 시작 실패 횟수
prometheus::Histogram& startup_duration_;        // 시작 소요 시간
prometheus::Counter& shm_retry_count_;           // 공유 메모리 재시도 횟수

// P2 - DataStore 버전 관리
prometheus::Counter& version_mismatch_count_;    // 버전 불일치 감지 횟수
prometheus::Histogram& version_check_latency_;   // 버전 체크 지연 시간

// P3 - EventBus 안정성
prometheus::Gauge& event_queue_usage_;           // 큐 사용률 (0-100%)
prometheus::Counter& events_dropped_;            // 버려진 이벤트 (Priority별)
prometheus::Counter& events_throttled_;          // Throttled 이벤트
prometheus::Counter& events_coalesced_;          // 병합된 이벤트

// P4 - HA Watchdog
prometheus::Counter& watchdog_heartbeats_;       // 하트비트 전송 횟수
prometheus::Counter& watchdog_timeouts_;         // Watchdog 타임아웃 횟수
prometheus::Counter& ipc_failures_;              // IPC 채널 장애 횟수
prometheus::Histogram& ipc_recovery_time_;       // IPC 복구 시간
```

**근거**: Feature 018 Prometheus 통합 (User Story 5)

---

**Phase 0 종합 결론**:

모든 NEEDS CLARIFICATION 항목이 해결되었습니다. 다음 Phase 1 (Design) 단계로 진행 가능합니다.

---

## Phase 1: Design & Contracts

### Data Model (data-model.md)

*참조: `docs/specs/022-fix-architecture-issues/data-model.md` (아래 생성)*

### API Contracts (contracts/)

*참조: `docs/specs/022-fix-architecture-issues/contracts/` (아래 생성)*

### Quickstart Guide (quickstart.md)

*참조: `docs/specs/022-fix-architecture-issues/quickstart.md` (아래 생성)*

---

## Constitution Check (Post-Design)

*Phase 1 설계 완료 후 재검증*

모든 GATE가 여전히 PASS 상태입니다:

- ✅ **GATE 1 (계층적 아키텍처)**: Phase 1 설계에서 새로운 계층 추가 없음, 기존 계층 내부 개선만 수행
- ✅ **GATE 2 (인터페이스 기반 설계)**: IDataAccessor, ISensorDataAccessor 등 모든 인터페이스 I-prefix 준수
- ✅ **GATE 3 (RAII)**: VersionedData는 스택 할당, Accessor는 참조만 보유, 스마트 포인터 사용
- ✅ **GATE 4 (메모리 안전성)**: AddressSanitizer 활성화, 수동 메모리 관리 없음
- ✅ **GATE 5 (TDD)**: 모든 우선순위에 대해 테스트 먼저 작성 계획 수립
- ✅ **GATE 6 (실시간 성능)**: Accessor 인라인 함수 (< 10ns), EventBus non-blocking (< 10μs)
- ✅ **GATE 7 (문서화)**: 모든 문서 한글 작성, 기술 용어만 영어

**종합 평가**: ✅ **ALL GATES PASSED (Post-Design)** - Phase 2 (Tasks) 진행 승인

---

## Next Steps

**Phase 2**: `/speckit.tasks` 명령 실행
- tasks.md 생성 (우선순위별 작업 분해)
- 각 작업에 대한 구체적인 구현 단계 정의
- 종속성 그래프 및 병렬 작업 식별

**예상 작업 개수**: 약 25-30개 작업
- P1 (Systemd 시작 순서): 5개 작업
- P2 (DataStore Accessor): 10개 작업
- P3 (EventBus 안정성): 8개 작업
- P4 (HA Watchdog): 4개 작업
- 공통 (문서화, 테스트): 5개 작업

---

**Plan 작성 완료**: 2025-01-22
**다음 명령**: `/speckit.tasks`
